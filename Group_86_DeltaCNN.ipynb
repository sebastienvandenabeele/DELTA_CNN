{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DeltaCNN Installation"
      ],
      "metadata": {
        "id": "wgP7QwKqQF5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to import the **deltacnn library**, we first need to clone the [DeltaCNN GitHub repository](https://github.com/facebookresearch/DeltaCNN).\n",
        "\n",
        "We then need to install the DeltaCNN framework. The README.md recommends to call the setup.py file directly using the following:\n",
        "```\n",
        "python setup.py install --user\n",
        "```\n",
        "However, we were not successful in installing the framework this way. Rather, we will **install using pip** directly on the DeltaCNN-Main folder."
      ],
      "metadata": {
        "id": "jIlbbFmrwA7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/DeltaCNN.git\n",
        "!pip install /content/DeltaCNN/ # original\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "W5yp33-DZVyW",
        "outputId": "56f9ab9d-ed39-4c99-bb60-648b9db0877a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeltaCNN' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./DeltaCNN\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchdeltacnn\n",
            "  Building wheel for torchdeltacnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchdeltacnn: filename=torchdeltacnn-0.0.0-cp39-cp39-linux_x86_64.whl size=17505296 sha256=fd1a28c83331af360ffe5118a044600157a3fc9d24882f3540d32d03a12fc40a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7r_wmfcj/wheels/79/07/be/b2e778efc1014ae6aaec3e3783a32762efddf8f6437493ea96\n",
            "Successfully built torchdeltacnn\n",
            "Installing collected packages: torchdeltacnn\n",
            "  Attempting uninstall: torchdeltacnn\n",
            "    Found existing installation: torchdeltacnn 0.0.0\n",
            "    Uninstalling torchdeltacnn-0.0.0:\n",
            "      Successfully uninstalled torchdeltacnn-0.0.0\n",
            "Successfully installed torchdeltacnn-0.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "deltacnn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now install the deltacnn package and other modules like torch, numpy, etc.:"
      ],
      "metadata": {
        "id": "HEoBsOsvwNbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import deltacnn\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from PIL import Image\n",
        "\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Additional setup to use Tensorboard\n",
        "!pip install -q tensorflow\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "mjLOshmxWj9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73452b27-e83f-4ec6-964f-4496ae23bd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Installation\n",
        "We load the MNIST dataset using torchvision and perform preprocessing on the dataset:"
      ],
      "metadata": {
        "id": "Ya2Nmamxw87R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIKsc7D2VP5_"
      },
      "outputs": [],
      "source": [
        "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
        "# mean and dividing by std.\n",
        "torch.manual_seed(0)\n",
        "pixel_permutation = torch.randperm(28*28)\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Get data from torchvision.datasets\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loaders used to iterate through dataset\n",
        "train_loader = DataLoader(train_data, batch_size= 8, drop_last=True, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = 8, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "Firstly, we setup some functions to compute the accuracy of the model for a given network and dataloader:"
      ],
      "metadata": {
        "id": "q2ai-T2LVi0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_train_accuracy(data_loader, net, device=torch.device('cpu')):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    #init\n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return acc_sum.item()/n"
      ],
      "metadata": {
        "id": "AAjFkC6nV9QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test_accuracy(data_loader, net, device=torch.device('cpu')):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    #init\n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return acc_sum.item()/n"
      ],
      "metadata": {
        "id": "NaEcqU5jb2FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the standard CNN model:"
      ],
      "metadata": {
        "id": "2GgGlVbKV-27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    3-layer CNN network with max pooling\n",
        "    \n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, hidden_channels, out_features):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, hidden_channels[0],\n",
        "                               kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels[0], hidden_channels[1],\n",
        "                               kernel_size=5,\n",
        "                               padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(7*7*hidden_channels[1], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        # Activation function\n",
        "        x = self.relu1(x)\n",
        "        # Max pool\n",
        "        x = self.max_pool1(x)\n",
        "        # Second convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        # Activation function\n",
        "        x = self.relu2(x)\n",
        "        # Max pool\n",
        "        x = self.max_pool2(x)\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Fully connected layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wuAxvfEejall"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's modify the standard architecture to use the DeltaCNN framework:"
      ],
      "metadata": {
        "id": "MgfH3-IEzPF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCNet(deltacnn.DCModule):\n",
        "    \"\"\"\n",
        "    3-layer CNN network with max pooling\n",
        "    \n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, hidden_channels, out_features):\n",
        "        super(DCNet, self).__init__()\n",
        "\n",
        "        self.conv1 = deltacnn.DCConv2d(in_channels, hidden_channels[0],\n",
        "                               kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.relu1 = deltacnn.DCActivation(activation=\"relu\")\n",
        "        self.max_pool1 = deltacnn.DCMaxPooling(2)\n",
        "        self.conv2 = deltacnn.DCConv2d(hidden_channels[0], hidden_channels[1],\n",
        "                               kernel_size=5,\n",
        "                               padding=2)\n",
        "        self.relu2 = deltacnn.DCActivation(activation=\"relu\")\n",
        "        self.max_pool2 = deltacnn.DCMaxPooling(2)\n",
        "        self.fc = nn.Linear(4056, out_features)\n",
        "\n",
        "        self.sparsify = deltacnn.DCSparsify()\n",
        "        self.densify = deltacnn.DCDensify()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Sparsify input\n",
        "        x = self.sparsify(x) \n",
        "        # First convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        # Activation function\n",
        "        x = self.relu1(x)\n",
        "        # Max pool\n",
        "        x = self.max_pool1(x)\n",
        "        # Second convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        # Activation function\n",
        "        x = self.relu2(x)\n",
        "        # Max pool\n",
        "        x = self.max_pool2(x)\n",
        "        # Densify output\n",
        "        x = self.densify(x)\n",
        "        # Flatten\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        # Fully connected layer\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZwXYa_QvWAUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train the selected model:"
      ],
      "metadata": {
        "id": "kxLAEbzCWSBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a writer to write to Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "in_channels = 1 # Black-white images in MNIST digits\n",
        "hidden_channels = [5, 6]\n",
        "out_features = 10 \n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.0015\n",
        "momentum = 0.9\n",
        "weight_decay = 0.001\n",
        "dampening = 0.2\n",
        "epochs = 10\n",
        "\n",
        "# Try using gpu instead of cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize network\n",
        "net = DCNet(in_channels, hidden_channels, out_features) # select which model to train\n",
        "net.to(device, memory_format=torch.channels_last) # set the network in channels last mode\n",
        "net.process_filters() # convert filters into DeltaCNN format\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay , dampening = dampening)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define list to store losses and performances of each iteration\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "overall_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Network in training mode and to device\n",
        "    net.train()\n",
        "    net.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "        # Set to same device\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Set the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform forward pass\n",
        "        y_pred = net(x_batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        # Backward computation and update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute train and test error\n",
        "    train_acc = 100*evaluate_train_accuracy(train_loader, net, device)\n",
        "    test_acc = 100*evaluate_test_accuracy(test_loader, net, device)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    writer.add_scalars('Accuracy', {'Train': train_acc, 'Test': test_acc}, epoch)\n",
        "    writer.add_scalars('Time', {'Train': epoch_time}, epoch)\n",
        "    \n",
        "    # Development of performance\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    # Print performance\n",
        "    print('Epoch: {:.0f}'.format(epoch+1))\n",
        "    print('Accuracy of train set: {:.2f}%'.format(train_acc))\n",
        "    print('Accuracy of test set: {:.2f}%'.format(test_acc))\n",
        "    print('')\n",
        "\n",
        "overall_time = time.time() - overall_start_time\n",
        "print('Overall time: {:.2f}'.format(overall_time))\n",
        "writer.flush()\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "rAjMhfIHWTTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a732268c-7ae2-4b5b-e4db-08e62f12c4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Accuracy of train set: 93.65%\n",
            "Accuracy of test set: 94.24%\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy of train set: 95.07%\n",
            "Accuracy of test set: 95.15%\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy of train set: 95.49%\n",
            "Accuracy of test set: 95.58%\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy of train set: 95.90%\n",
            "Accuracy of test set: 95.84%\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy of train set: 96.20%\n",
            "Accuracy of test set: 96.16%\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy of train set: 96.30%\n",
            "Accuracy of test set: 96.09%\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy of train set: 96.32%\n",
            "Accuracy of test set: 96.29%\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy of train set: 96.39%\n",
            "Accuracy of test set: 96.24%\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy of train set: 96.62%\n",
            "Accuracy of test set: 96.41%\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy of train set: 96.68%\n",
            "Accuracy of test set: 96.52%\n",
            "\n",
            "Epoch: 11\n",
            "Accuracy of train set: 96.62%\n",
            "Accuracy of test set: 96.56%\n",
            "\n",
            "Epoch: 12\n",
            "Accuracy of train set: 96.77%\n",
            "Accuracy of test set: 96.55%\n",
            "\n",
            "Epoch: 13\n",
            "Accuracy of train set: 96.88%\n",
            "Accuracy of test set: 96.56%\n",
            "\n",
            "Epoch: 14\n",
            "Accuracy of train set: 96.78%\n",
            "Accuracy of test set: 96.77%\n",
            "\n",
            "Epoch: 15\n",
            "Accuracy of train set: 96.89%\n",
            "Accuracy of test set: 96.81%\n",
            "\n",
            "Epoch: 16\n",
            "Accuracy of train set: 96.78%\n",
            "Accuracy of test set: 96.69%\n",
            "\n",
            "Epoch: 17\n",
            "Accuracy of train set: 96.92%\n",
            "Accuracy of test set: 96.64%\n",
            "\n",
            "Epoch: 18\n",
            "Accuracy of train set: 96.89%\n",
            "Accuracy of test set: 96.61%\n",
            "\n",
            "Epoch: 19\n",
            "Accuracy of train set: 96.84%\n",
            "Accuracy of test set: 96.87%\n",
            "\n",
            "Epoch: 20\n",
            "Accuracy of train set: 96.84%\n",
            "Accuracy of test set: 96.59%\n",
            "\n",
            "Epoch: 21\n",
            "Accuracy of train set: 96.91%\n",
            "Accuracy of test set: 96.73%\n",
            "\n",
            "Epoch: 22\n",
            "Accuracy of train set: 96.86%\n",
            "Accuracy of test set: 96.80%\n",
            "\n",
            "Epoch: 23\n",
            "Accuracy of train set: 96.84%\n",
            "Accuracy of test set: 96.53%\n",
            "\n",
            "Epoch: 24\n",
            "Accuracy of train set: 97.01%\n",
            "Accuracy of test set: 96.58%\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a5a08bd5e6df>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Set to same device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw the learning curves:"
      ],
      "metadata": {
        "id": "RvgWqwZgWaB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = [t.cpu().detach().numpy() for t in train_losses]\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(train_accs, label = 'train')\n",
        "plt.plot(test_accs, label = 'test')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "5QsyfkleWcLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize results with Tensorboard:"
      ],
      "metadata": {
        "id": "keV2ZKuBh5Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "metadata": {
        "id": "KH9DUFsnh7je"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}